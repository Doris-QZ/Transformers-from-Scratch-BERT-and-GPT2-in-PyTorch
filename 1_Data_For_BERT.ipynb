{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Doris-QZ/Transformers-from-Scratch-BERT-and-GPT2-in-PyTorch/blob/main/1_Data_For_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction\n",
        "\n",
        "This notebook prepares the **IMDB dataset** for BERT-style pre-training. The data is processed to align with **BERT’s training objectives**:\n",
        "\n",
        "* **Masked Language Modeling (MLM)**: randomly masks tokens for prediction.\n",
        "* **Next Sentence Prediction (NSP)**: create paired sentences with both positive and negative examples.  \n",
        ">\n",
        "\n",
        "At the end of this notebook, we'll have a pandas DataFrame with the following columns:\n",
        "* **input_ids** - token IDs of paired sentences       \n",
        "* **token_type_ids** - token type IDs (0 for the first sentence, 1 for the second)\n",
        "* **attention_mask** - 1 for real tokens, 0 for padding\n",
        "* **mlm_labels** - labels for masked language model prediction\n",
        "* **nsp_labels** - labels for next sentence prediction.  \n",
        ">\n",
        "   \n",
        "Although this is not the same corpus used in the original BERT paper (BooksCorpus + Wikipedia), the dataset is structured in a way that allows the model to be trained with the same objectives.  \n"
      ],
      "metadata": {
        "id": "EE8tNqYC23Xm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import transformers\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "import pandas as pd\n",
        "import random"
      ],
      "metadata": {
        "id": "xKa-gL4Z4_q4"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Suppress the warning\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "\n",
        "# Load the data\n",
        "imdb_data = load_dataset('imdb', split='train')"
      ],
      "metadata": {
        "id": "kQwTagUg6gvw"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the data\n",
        "print(f\"{imdb_data}\\n\")\n",
        "imdb_data[0]"
      ],
      "metadata": {
        "id": "P1hngl8m6HrE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d53c62bb-adac-4c4c-b3b5-0c3dfb66d0e6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['text', 'label'],\n",
            "    num_rows: 25000\n",
            "})\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.',\n",
              " 'label': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT uses **WordPiece**, a subword tokenization algorithm, with a vocabulary size of 30,522 in the BERT-base model. We’ll load the BERT-base tokenizer from Hugging Face’s transformers package for text tokenization.\n",
        ">\n",
        "**Note:** Although the BERT tokenizer can directly return `input_ids`, `attention_mask`, and `token_type_ids`, in this project we only use its `encode` method to obtain token IDs. We will then **create `input_ids`, `attention_mask`, and `token_type_ids` manually**, since our goal is to **explore how data is prepared for masked language modeling (MLM) and next sentence prediction (NSP)**."
      ],
      "metadata": {
        "id": "qWgSMxei1ZQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load BERT-base tokenizer\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Get vocabulary and vocabulary size\n",
        "vocab = bert_tokenizer.get_vocab()\n",
        "vocab_size = len(vocab)"
      ],
      "metadata": {
        "id": "EcMedaBVGmYk"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function for Preparing Data for Masked Language Modeling (MLM)\n",
        "According to the BERT paper, **15% of tokens are randomly selected for masking**.  \n",
        "\n",
        "For example,<br>\n",
        "\n",
        "> Original text: \"my dog is very cute.\"  \n",
        "> Randomly selected token: \"cute\"\n",
        "\n",
        "For the selected tokens:\n",
        "* 80% of time ---> replaced with [MASK] ---> \"my dog is very **[MASK]**.\"\n",
        "* 10% of time ---> left unchanged ---> \"my dog is very **cute**.\"\n",
        "* 10% of time ---> replaced with a random token ---> \"my dog is very **apple**.\"    \n",
        ">\n",
        "\n",
        "Regardless of how the selected tokens are masked, the label remains the same:\n",
        "> Label: [PAD][PAD][PAD][PAD] cute.  \n",
        "\n",
        "During training, the model **ignores the [PAD] tokens** and **predicts the original tokens** (in this case, \"cute\") at the selected positions."
      ],
      "metadata": {
        "id": "5SyaYdlqLoX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get special token indices\n",
        "PAD_IDX = bert_tokenizer.pad_token_id\n",
        "UNK_IDX = bert_tokenizer.unk_token_id\n",
        "CLS_IDX = bert_tokenizer.cls_token_id\n",
        "SEP_IDX = bert_tokenizer.sep_token_id\n",
        "MASK_IDX = bert_tokenizer.mask_token_id\n",
        "\n",
        "PAD_IDX, UNK_IDX, CLS_IDX, SEP_IDX, MASK_IDX"
      ],
      "metadata": {
        "id": "oMSEdHmwvbjF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4936a4de-0f24-4133-c887-32c91d6dfcf8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 100, 101, 102, 103)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "special_ids = {PAD_IDX, UNK_IDX, CLS_IDX, SEP_IDX, MASK_IDX}\n",
        "\n",
        "def random_token(vocab_size: int = vocab_size,\n",
        "                 special_ids: set[int] | list[int] = special_ids) -> int:\n",
        "    \"\"\"\n",
        "    Returns a random token ID that is not a special token.\n",
        "    \"\"\"\n",
        "    while True:\n",
        "        idx = random.randint(0, vocab_size-1)\n",
        "        if idx not in special_ids:\n",
        "            return idx\n",
        "\n",
        "def masking(token_id: int, pad_idx: int, mask_idx: int) -> tuple[int, int]:\n",
        "    \"\"\"\n",
        "    Mask a single token for Masked Language Model (MLM) training.\n",
        "\n",
        "    Args:\n",
        "        token_id (int): The token_id to be processed.\n",
        "        pad_idx (int): Index of the [PAD] token.\n",
        "        mask_idx (int): Index of the [MASK] token.\n",
        "\n",
        "    Returns:\n",
        "        tuple[int, int]: A tuple containing:\n",
        "            1. The processed token id (masked, random, or original).\n",
        "            2. The label for the token (original token id if masked, or pad_idx)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # The probability of a token being masked is 15%.\n",
        "    mask = random.random() <= 0.15\n",
        "\n",
        "    if not mask:\n",
        "        token_ = token_id\n",
        "        label_ = pad_idx\n",
        "        return token_, label_\n",
        "\n",
        "    # Generates a random float between 0 and 1\n",
        "    random_float = random.random()\n",
        "\n",
        "    # 80% of the selected tokens will be repalced by mask_idx\n",
        "    if random_float < 0.8:\n",
        "        token_ = mask_idx\n",
        "        label_ = token_id\n",
        "        return token_, label_\n",
        "\n",
        "    # 10% of the selected tokens will remain unchanged\n",
        "    if random_float > 0.9:\n",
        "        token_ = token_id\n",
        "        label_ = token_id\n",
        "        return token_, label_\n",
        "\n",
        "    # 10% of the selected tokens will be replaced by a random token id\n",
        "    else:\n",
        "        token_ = random_token()\n",
        "        label_ = token_id\n",
        "        return token_, label_"
      ],
      "metadata": {
        "id": "t5_R9cbvLVXH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "PUNCT_IDX = {vocab['.'], vocab['?'], vocab['!']}\n",
        "\n",
        "def data_for_MLM(dataset: Dataset,\n",
        "                 tokenizer,\n",
        "                 pad_idx: int,\n",
        "                 mask_idx: int,\n",
        "                 punct_idx: set[int] | list[int]) -> tuple[list, list]:\n",
        "    \"\"\"\n",
        "    Prepare data for Masked Language model (MLM) training.\n",
        "\n",
        "    Args:\n",
        "        dataset (Dataset): The dataset to be processed.\n",
        "        tokenizer: The tokenizer to encode the text.\n",
        "        pad_idx (int): Index of the [PAD] token.\n",
        "        mask_idx (int): Index of the [MASK] token.\n",
        "        punct_idx (set | list): A set or list of indices of ending punctuations\n",
        "\n",
        "    Returns:\n",
        "        tuple[list, list]: A tuple containing:\n",
        "            1. List of tokenized and masked sentences\n",
        "            2. List of labels corresponding to the masked sentences.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    masked_sentences = []\n",
        "    mlm_labels = []\n",
        "\n",
        "    for data in dataset:\n",
        "        cur_tokens = []\n",
        "        cur_labels = []\n",
        "        tokens =  tokenizer.encode(data['text'], add_special_tokens=False)\n",
        "\n",
        "        for token_id in tokens:\n",
        "            token_, label_ = masking(token_id, pad_idx, mask_idx)\n",
        "            cur_tokens.append(token_)\n",
        "            cur_labels.append(label_)\n",
        "\n",
        "            # Found a token indicates the end of sentence, process the sentence and reset it.\n",
        "            if token_id in punct_idx:\n",
        "                if len(cur_tokens) > 2:\n",
        "                    masked_sentences.append(cur_tokens)\n",
        "                    mlm_labels.append(cur_labels)\n",
        "\n",
        "                # Reset for the next sentence within the same document\n",
        "                cur_tokens = []\n",
        "                cur_labels = []\n",
        "\n",
        "        # Append leftovers (if the document didn't end with ending punctuations)\n",
        "        if cur_tokens:\n",
        "            masked_sentences.append(cur_tokens)\n",
        "            mlm_labels.append(cur_labels)\n",
        "\n",
        "    return masked_sentences, mlm_labels\n"
      ],
      "metadata": {
        "id": "IeN1Rf9XFwWX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function for Preparing Data for Next Sentence Prediction (NSP)\n",
        "\n",
        "BERT is trained to predict whether a pair of sentences are consecutive in the original text (**Next Sentence Prediction**).\n",
        "\n",
        "* **Positive examples**: the second sentence follows the first sentence in the dataset.\n",
        "\n",
        "* **Negative examples**: the second sentence is randomly selected from the dataset.\n",
        "\n",
        "For each sentence pair, a label is created:\n",
        "\n",
        "* 1 → Next sentence is correct (positive example)\n",
        "\n",
        "* 0 → Next sentence is incorrect (negative example)\n",
        "\n",
        "This allows the model to learn relationships between sentences in addition to word-level predictions."
      ],
      "metadata": {
        "id": "FFDpmLKYFbCs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_for_NSP(masked_sentences: list[list[int]],\n",
        "                 mlm_labels: list[list[int]],\n",
        "                 pad_idx: int,\n",
        "                 cls_idx: int,\n",
        "                 sep_idx: int) -> tuple[list, list, list]:\n",
        "    \"\"\"\n",
        "    Prepare data for Next Sentence Prediction (NSP).\n",
        "\n",
        "    Args:\n",
        "        masked_sentences (list[list[int]]): List of masked sentences.\n",
        "        mlm_labels (list[list[int]]): List of labels corresponding to masked sentences.\n",
        "        pad_idx (int): Index of the [PAD] token.\n",
        "        cls_idx (int): Index of the [CLS] token.\n",
        "        sep_idx (int): Index of the [SEP] token.\n",
        "\n",
        "    Returns:\n",
        "        tuple[list, list, list]: A tuple containing:\n",
        "            1. List of paired sentences with special token indices added.\n",
        "            2. List of labels for masked token.\n",
        "            3. List of NSP labels (1 for consecutive, 0 for random).\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    # Make sure the length of inputs are valid\n",
        "    num_sentence = len(masked_sentences)\n",
        "    if num_sentence < 2:\n",
        "        raise ValueError(\"Must be more than two sentences in the input_tokens.\")\n",
        "\n",
        "    if num_sentence != len(mlm_labels):\n",
        "        raise ValueError(\"The input_tokens and the labels must have the same length.\")\n",
        "\n",
        "\n",
        "    paired_sents = []\n",
        "    paired_mlm_labels = []\n",
        "    nsp_labels = []\n",
        "\n",
        "    # Create the list of sentence indices\n",
        "    sentence_idx = list(range(num_sentence))\n",
        "\n",
        "    while len(sentence_idx) >= 2:\n",
        "\n",
        "        # ---Positive Pair (consecutive)---\n",
        "        if random.random() >= 0.5:\n",
        "\n",
        "            # Pick a starter index\n",
        "            idx = random.choice(sentence_idx[:-1])\n",
        "\n",
        "            # Pair (idx) and (idx+1) with special tokens as current inputs/labels\n",
        "            cur_input = [[cls_idx] + masked_sentences[idx] + [sep_idx],\n",
        "                          masked_sentences[idx + 1] + [sep_idx]]\n",
        "            cur_label = [[pad_idx] + mlm_labels[idx] + [pad_idx],\n",
        "                          mlm_labels[idx + 1] + [pad_idx]]\n",
        "\n",
        "            paired_sents.append(cur_input)\n",
        "            paired_mlm_labels.append(cur_label)\n",
        "            nsp_labels.append(1)      # Append 1 to nsp_label, indicating 'IsNext'\n",
        "\n",
        "            # Remove idx. Also remove idx+1 if it is still available\n",
        "            sentence_idx.remove(idx)\n",
        "            if (idx+1) in sentence_idx:\n",
        "                sentence_idx.remove(idx+1)\n",
        "\n",
        "        # ---Negative Pair (random)---\n",
        "        else:\n",
        "\n",
        "            # Pick two random indices\n",
        "            idx_1, idx_2 = random.sample(sentence_idx, 2)\n",
        "\n",
        "            # Make sure the two indices are NOT neighbors\n",
        "            attempts = 0\n",
        "            while abs(idx_1 - idx_2) == 1 and attempts < 5:\n",
        "                idx_1, idx_2 = random.sample(sentence_idx, 2)\n",
        "                attempts += 1\n",
        "\n",
        "            # If the two indices are still neighbors after 5 tries, skip the iteration to avoid bad data.\n",
        "            if abs(idx_1 - idx_2) == 1:\n",
        "                continue\n",
        "\n",
        "            cur_input = [[cls_idx] + masked_sentences[idx_1] + [sep_idx],\n",
        "                         masked_sentences[idx_2] + [sep_idx]]\n",
        "            cur_label = [[pad_idx] + mlm_labels[idx_1] + [pad_idx],\n",
        "                         mlm_labels[idx_2] + [pad_idx]]\n",
        "\n",
        "            paired_sents.append(cur_input)\n",
        "            paired_mlm_labels.append(cur_label)\n",
        "            nsp_labels.append(0)    # Append 0 to nsp_label, indicating 'NotNext'\n",
        "\n",
        "            # Remove both index\n",
        "            sentence_idx.remove(idx_1)\n",
        "            sentence_idx.remove(idx_2)\n",
        "\n",
        "    return paired_sents, paired_mlm_labels, nsp_labels"
      ],
      "metadata": {
        "id": "swNZTkrClkgG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function to Create Final BERT Training Data\n",
        "\n",
        "The final function combines the **MLM** and **NSP** data preparation steps:\n",
        "\n",
        "1. Randomly masks 15% of tokens and prepares the corresponding labels for **Masked Language Modeling (MLM)**.\n",
        "\n",
        "2. Generates sentence pairs and labels for **Next Sentence Prediction (NSP)**.\n",
        "\n",
        "3. Returns a **pandas DataFrame** containing all the processed data.\n",
        ">\n",
        "\n",
        "The output DataFrame can then be saved as a CSV file, which is later loaded in the `Reproducing BERT Model from Scratch using PyTorch.ipynb` notebook. In that notebook, the data is **converted to a PyTorch dataset** and loaded into a **PyTorch DataLoader** for training."
      ],
      "metadata": {
        "id": "jUBXZJdt1ULt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_for_BERT(dataset: Dataset,\n",
        "                  tokenizer,\n",
        "                  pad_idx: int,\n",
        "                  mask_idx: int,\n",
        "                  cls_idx: int,\n",
        "                  sep_idx: int,\n",
        "                  punct_idx: set[int] | list[int]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Prepare data for BERT training--Masked Language Modeling and Next Sentence Prediction.\n",
        "\n",
        "    Args:\n",
        "        dataset (Dataset): The dataset to be processed.\n",
        "        tokenizer: The tokenizer to encode the text.\n",
        "        pad_idx (int): Index of the [PAD] token.\n",
        "        mask_idx (int): Index of the [MASK] token.\n",
        "        cls_idx (int): Index of the [CLS] token.\n",
        "        sep_idx (int): Index of the [SEP] token.\n",
        "        punct_idx (set | list): A set or list of indices of ending punctuations.\n",
        "\n",
        "    Returns:\n",
        "        A Pandas dataframe with the following columns:\n",
        "            input_ids - token IDs of paired sentences\n",
        "            token_type_ids - token type IDs (0 for the first sentence, 1 for the second)\n",
        "            attention_mask - 1 for real tokens, 0 for padding\n",
        "            mlm_labels - labels for masked language model prediction\n",
        "            nsp_labels - labels for next sentence prediction\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Get masked_sentences and the corresponding labels from the dataset\n",
        "    masked_sentences, mask_labels = data_for_MLM(dataset, tokenizer, pad_idx, mask_idx, punct_idx)\n",
        "\n",
        "    # Get paired_sentences, labels, and nsp_labels list\n",
        "    paired_sentences, paired_mask_labels, nsp_labels = data_for_NSP(masked_sentences, mask_labels, pad_idx, cls_idx, sep_idx)\n",
        "\n",
        "    input_ids, token_type_ids, attention_mask, mlm_labels = [], [], [], []\n",
        "\n",
        "    for sentences, labels in zip(paired_sentences, paired_mask_labels):\n",
        "\n",
        "        # flatten paired sentences and mask_labels\n",
        "        padded_sent = sentences[0] + sentences[1]\n",
        "        padded_label = labels[0] + labels[1]\n",
        "\n",
        "        # Create token types (0 for the first sentence, 1 for the second)\n",
        "        token_type = [0] * len(sentences[0]) + [1] * len(sentences[1])\n",
        "\n",
        "        # Create attention mask (1 for real tokens)\n",
        "        mask = [1] * len(sentences[0]) + [1] * len(sentences[1])\n",
        "\n",
        "        # Append data to final lists\n",
        "        input_ids.append(padded_sent)\n",
        "        mlm_labels.append(padded_label)\n",
        "        token_type_ids.append(token_type)\n",
        "        attention_mask.append(mask)\n",
        "\n",
        "    # Create dataframe\n",
        "    bert_data = pd.DataFrame({\n",
        "        'input_ids': input_ids,\n",
        "        'token_type_ids': token_type_ids,\n",
        "        'attention_mask': attention_mask,\n",
        "        'mlm_labels': mlm_labels,\n",
        "        'nsp_labels': nsp_labels\n",
        "    })\n",
        "\n",
        "    return bert_data"
      ],
      "metadata": {
        "id": "DLl-LunGw_z7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process imdb_data for BERT training\n",
        "imdb_bert_data = data_for_BERT(imdb_data, bert_tokenizer, PAD_IDX, MASK_IDX, CLS_IDX, SEP_IDX, PUNCT_IDX)\n",
        "imdb_bert_data.head()"
      ],
      "metadata": {
        "id": "_YqjQLFwtMPx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "90ec73c1-b0ec-4395-f819-880ea1dffa0c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (718 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                           input_ids  \\\n",
              "0  [101, 11387, 2005, 1996, 3787, 1997, 3348, 101...   \n",
              "1  [101, 2010, 3566, 2003, 1037, 16299, 103, 1996...   \n",
              "2  [101, 3100, 1010, 1045, 2001, 11471, 1998, 103...   \n",
              "3  [101, 1998, 1045, 1005, 103, 2469, 2008, 2065,...   \n",
              "4  [101, 1045, 2074, 2293, 1996, 10256, 4038, 199...   \n",
              "\n",
              "                                      token_type_ids  \\\n",
              "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...   \n",
              "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
              "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, ...   \n",
              "\n",
              "                                      attention_mask  \\\n",
              "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
              "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
              "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
              "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
              "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
              "\n",
              "                                          mlm_labels  nsp_labels  \n",
              "0  [0, 2298, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...           1  \n",
              "1  [0, 0, 0, 0, 0, 0, 1998, 0, 0, 0, 0, 0, 2028, ...           1  \n",
              "2  [0, 0, 0, 0, 0, 11471, 0, 2787, 0, 0, 0, 0, 0,...           0  \n",
              "3  [0, 0, 0, 0, 1049, 0, 0, 0, 0, 0, 2009, 0, 0, ...           0  \n",
              "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...           0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-faee84bb-f5e7-439f-9fc2-89ca70774669\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>input_ids</th>\n",
              "      <th>token_type_ids</th>\n",
              "      <th>attention_mask</th>\n",
              "      <th>mlm_labels</th>\n",
              "      <th>nsp_labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[101, 11387, 2005, 1996, 3787, 1997, 3348, 101...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[0, 2298, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[101, 2010, 3566, 2003, 1037, 16299, 103, 1996...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 1998, 0, 0, 0, 0, 0, 2028, ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[101, 3100, 1010, 1045, 2001, 11471, 1998, 103...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 11471, 0, 2787, 0, 0, 0, 0, 0,...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[101, 1998, 1045, 1005, 103, 2469, 2008, 2065,...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[0, 0, 0, 0, 1049, 0, 0, 0, 0, 0, 2009, 0, 0, ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[101, 1045, 2074, 2293, 1996, 10256, 4038, 199...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, ...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-faee84bb-f5e7-439f-9fc2-89ca70774669')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-faee84bb-f5e7-439f-9fc2-89ca70774669 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-faee84bb-f5e7-439f-9fc2-89ca70774669');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-fcf4964b-2cf3-47ae-a013-0a74b37e74ef\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fcf4964b-2cf3-47ae-a013-0a74b37e74ef')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-fcf4964b-2cf3-47ae-a013-0a74b37e74ef button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "imdb_bert_data"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imdb_bert_data.info()"
      ],
      "metadata": {
        "id": "T0ZcWDYXa0B9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "502fe420-1fa8-4817-a0e3-decd02f3c0f4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 185667 entries, 0 to 185666\n",
            "Data columns (total 5 columns):\n",
            " #   Column          Non-Null Count   Dtype \n",
            "---  ------          --------------   ----- \n",
            " 0   input_ids       185667 non-null  object\n",
            " 1   token_type_ids  185667 non-null  object\n",
            " 2   attention_mask  185667 non-null  object\n",
            " 3   mlm_labels      185667 non-null  object\n",
            " 4   nsp_labels      185667 non-null  int64 \n",
            "dtypes: int64(1), object(4)\n",
            "memory usage: 7.1+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the dataframe as CSV\n",
        "imdb_bert_data.to_csv('imdb_bert_data.csv', index=False)"
      ],
      "metadata": {
        "id": "QdP5h8tra6yJ"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}